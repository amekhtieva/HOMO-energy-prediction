{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a5090",
   "metadata": {
    "id": "6f6a5090",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Simple Impementation of E(n) Equivariant Graph Neural Networks\n",
    "\n",
    "Original paper https://arxiv.org/pdf/2102.09844.pdf by Victor Garcia Satorras, Emiel Hoogeboom, Max Welling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bU4ixrOJCg1",
   "metadata": {
    "id": "4bU4ixrOJCg1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb08a10",
   "metadata": {
    "id": "8cb08a10",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load QM9 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae30de9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae30de9d",
    "outputId": "aa32fe39-7dc3-4770-b207-f24b55ba7bd2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'simple-equivariant-gnn' already exists and is not an empty directory.\r\n",
      "/home/leonid/Загрузки/Telegram Desktop/simple-equivariant-gnn\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/senya-ashukha/simple-equivariant-gnn.git\n",
    "%cd simple-equivariant-gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859f981c",
   "metadata": {
    "id": "859f981c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# QM9 is a dataset for Molecular Property Predictions http://quantum-machine.org/datasets/\n",
    "# We will predict Highest occupied molecular orbital energy \n",
    "# https://en.wikipedia.org/wiki/HOMO_and_LUMO\n",
    "# We use data loaders from the official repo\n",
    "\n",
    "from qm9.data_utils import get_data, BatchGraph\n",
    "train_loader, val_loader, test_loader, charge_scale = get_data(num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e20004",
   "metadata": {
    "id": "05e20004",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Graph Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0acbcc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0acbcc0",
    "outputId": "fa546964-93a4-4fb0-b2e3-918918ae4d87",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In the batch: num_graphs 96 num_nodes 1749\n",
       "> .h \t\t a tensor of nodes representations \t\tshape 1749 x 15\n",
       "> .x \t\t a tensor of nodes positions  \t\t\tshape 1749 x 3\n",
       "> .edges \t a tensor of edges, a fully connected graph \tshape 31134 x 2\n",
       "> .batch  \t a tensor of graph_ids for each node \t\ttensor([ 0,  0,  0,  ..., 95, 95, 95])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = BatchGraph(iter(train_loader).next(), False, charge_scale)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c0726",
   "metadata": {
    "id": "784c0726",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Define Equivariant Graph Convs  & GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e5e05f",
   "metadata": {
    "id": "76e5e05f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def index_sum(agg_size, source, idx, cuda):\n",
    "    \"\"\"\n",
    "        source is N x hid_dim [float]\n",
    "        idx    is N           [int]\n",
    "        \n",
    "        Sums the rows source[.] with the same idx[.];\n",
    "    \"\"\"\n",
    "    tmp = torch.zeros((agg_size, source.shape[1]))\n",
    "    tmp = tmp.cuda() if cuda else tmp\n",
    "    res = torch.index_add(tmp, 0, idx, source)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5d55db",
   "metadata": {
    "id": "4d5d55db",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConvEGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, cuda=True):\n",
    "        super().__init__()\n",
    "        self.hid_dim=hid_dim\n",
    "        self.cuda = cuda\n",
    "        \n",
    "        #### **** YOUR CODE HERE **** #####\n",
    "        # computes messages based on hidden representations -> [0, 1]\n",
    "        self.f_e = nn.Sequential(\n",
    "            nn.Linear(2 * in_dim + 1, hid_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.SiLU()\n",
    "            )\n",
    "\n",
    "        # Tips: See eq. 3 from the paper.\n",
    "        # Also use nn.Sequential\n",
    "        \n",
    "        #### **** YOUR CODE HERE **** #####\n",
    "        # preducts \"soft\" edges based on messages \n",
    "        self.f_inf = nn.Sequential(\n",
    "            nn.Linear(hid_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        # Tips: See eq. 8 from the paper.\n",
    "        \n",
    "        #### **** YOUR CODE HERE **** #####\n",
    "        # updates hidden representations -> [0, 1]\n",
    "        self.f_h = nn.Sequential(\n",
    "            nn.Linear(in_dim + hid_dim, hid_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim)\n",
    "            )\n",
    "\n",
    "        # Tips: See eq. 6 from the paper.\n",
    "    \n",
    "    def forward(self, b):\n",
    "        #### **** YOUR CODE HERE **** #####\n",
    "        # compute distances for all edges\n",
    "        e_st, e_end = b.edges[:,0], b.edges[:,1]\n",
    "        dists = torch.norm(b.x[e_st] - b.x[e_end], dim=1).reshape((-1, 1))\n",
    "        # dist should be a 2d torch.Tensor of shape (#number_of_edges_in_the_batch, 1)\n",
    "        \n",
    "        #### **** YOUR CODE HERE **** #####\n",
    "        # compute messages\n",
    "        tmp = torch.hstack([b.h[e_st], b.h[e_end], dists])\n",
    "        m_ij = self.f_e(tmp).reshape((-1, self.hid_dim))\n",
    "        # m_ij should be a 2d torch.Tensor of shape (#number_of_edges_in_the_batch, hid_dim)\n",
    "        \n",
    "        # predict edges\n",
    "        e_ij = self.f_inf(m_ij)\n",
    "        \n",
    "        # average e_ij-weighted messages  \n",
    "        # m_i is num_nodes x hid_dim\n",
    "        m_i = index_sum(b.h.shape[0], e_ij*m_ij, b.edges[:,0], self.cuda)\n",
    "        \n",
    "        #### **** YOUR CODE HERE **** #####\n",
    "        # update hidden representations\n",
    "        b.h = torch.reshape(self.f_h(torch.hstack([b.h, m_i])) + b.h, (-1, self.hid_dim))\n",
    "        # b.h should be a 2d torch.Tensor of shape (#number_of_nodes_in_the_batch, hid_dim)\n",
    "        # see appendix C. Implementatation details\n",
    "\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10aad7c4",
   "metadata": {
    "id": "10aad7c4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NetEGNN(nn.Module):\n",
    "    def __init__(self, in_dim=15, hid_dim=128, out_dim=1, n_layers=7, cuda=True):\n",
    "        super().__init__()\n",
    "        self.hid_dim=hid_dim\n",
    "        \n",
    "        self.emb = nn.Linear(in_dim, hid_dim) \n",
    "\n",
    "        #### **** YOUR CODE HERE **** #####\n",
    "        # Make gnn of n_layers\n",
    "        self.gnn = [ConvEGNN(hid_dim, hid_dim, cuda=cuda) for _ in range(n_layers)]\n",
    "        self.gnn = nn.Sequential(*self.gnn)\n",
    "        # use ConvEGNN layer\n",
    "\n",
    "        \n",
    "        \n",
    "        self.pre_mlp = nn.Sequential(\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim))\n",
    "        \n",
    "        self.post_mlp = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, out_dim))\n",
    "\n",
    "        if cuda: self.cuda()\n",
    "        self.cuda = cuda\n",
    "    \n",
    "    def forward(self, b):\n",
    "        b.h = self.emb(b.h)\n",
    "        \n",
    "        b = self.gnn(b)\n",
    "        h_nodes = self.pre_mlp(b.h)\n",
    "        \n",
    "        # h_graph is num_graphs x hid_dim\n",
    "        h_graph = index_sum(b.nG, h_nodes, b.batch, self.cuda) \n",
    "        \n",
    "        out = self.post_mlp(h_graph)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f4cef6",
   "metadata": {
    "id": "b7f4cef6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "cuda = True\n",
    "\n",
    "model = NetEGNN(n_layers=7, cuda=cuda)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-16)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d6b1c",
   "metadata": {
    "id": "4e5d6b1c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de3613c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de3613c9",
    "outputId": "e8098bc7-df4c-4146-b9f5-c3f8bed76cc7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> start training\n",
      "> epoch 000: train 391.683 val 322.151 test 321.186 (130.6 sec)\n",
      "> epoch 001: train 281.738 val 244.154 test 241.878 (129.7 sec)\n",
      "> epoch 002: train 223.642 val 205.746 test 204.311 (129.7 sec)\n",
      "> epoch 003: train 198.256 val 195.704 test 193.652 (128.0 sec)\n",
      "> epoch 004: train 180.583 val 185.659 test 185.232 (128.1 sec)\n",
      "> epoch 005: train 166.743 val 173.664 test 171.944 (128.0 sec)\n",
      "> epoch 006: train 154.025 val 138.538 test 138.583 (127.8 sec)\n",
      "> epoch 007: train 142.391 val 133.925 test 131.565 (128.3 sec)\n",
      "> epoch 008: train 133.180 val 122.881 test 123.177 (127.9 sec)\n",
      "> epoch 009: train 126.694 val 116.769 test 118.137 (129.0 sec)\n",
      "> epoch 010: train 122.175 val 116.368 test 116.089 (131.9 sec)\n",
      "> epoch 011: train 115.751 val 113.866 test 113.141 (131.0 sec)\n",
      "> epoch 012: train 111.131 val 111.591 test 112.609 (131.5 sec)\n",
      "> epoch 013: train 108.285 val 98.241 test 98.364 (129.0 sec)\n",
      "> epoch 014: train 103.581 val 115.931 test 114.751 (129.1 sec)\n",
      "> epoch 015: train 100.250 val 92.349 test 91.743 (129.2 sec)\n",
      "> epoch 016: train 97.299 val 92.687 test 90.830 (129.9 sec)\n",
      "> epoch 017: train 94.619 val 95.379 test 94.487 (129.7 sec)\n",
      "> epoch 018: train 92.574 val 89.071 test 89.037 (129.5 sec)\n",
      "> epoch 019: train 91.228 val 85.662 test 84.967 (130.1 sec)\n",
      "> epoch 020: train 89.241 val 85.594 test 84.688 (130.1 sec)\n",
      "> epoch 021: train 87.678 val 92.174 test 92.051 (130.3 sec)\n",
      "> epoch 022: train 90.048 val 80.866 test 80.492 (133.0 sec)\n",
      "> epoch 023: train 83.003 val 82.217 test 83.109 (132.1 sec)\n",
      "> epoch 024: train 82.594 val 83.618 test 83.871 (132.0 sec)\n",
      "> epoch 025: train 80.962 val 76.582 test 77.972 (129.7 sec)\n",
      "> epoch 026: train 80.546 val 80.503 test 80.238 (129.5 sec)\n",
      "> epoch 027: train 78.973 val 80.017 test 80.137 (129.5 sec)\n",
      "> epoch 028: train 78.583 val 74.553 test 75.653 (129.8 sec)\n",
      "> epoch 029: train 76.798 val 76.160 test 74.922 (129.4 sec)\n",
      "> epoch 030: train 76.181 val 70.994 test 71.288 (129.8 sec)\n",
      "> epoch 031: train 76.095 val 73.781 test 74.281 (130.0 sec)\n",
      "> epoch 032: train 74.534 val 78.762 test 79.784 (130.0 sec)\n",
      "> epoch 033: train 73.258 val 74.218 test 74.973 (130.0 sec)\n",
      "> epoch 034: train 72.458 val 76.005 test 75.505 (129.8 sec)\n",
      "> epoch 035: train 73.003 val 68.798 test 69.017 (129.9 sec)\n",
      "> epoch 036: train 71.389 val 66.860 test 67.781 (129.5 sec)\n",
      "> epoch 037: train 70.223 val 70.824 test 71.262 (129.5 sec)\n",
      "> epoch 038: train 69.665 val 68.585 test 68.212 (129.2 sec)\n",
      "> epoch 039: train 68.831 val 67.931 test 69.089 (129.6 sec)\n",
      "> epoch 040: train 67.529 val 68.529 test 68.670 (129.3 sec)\n",
      "> epoch 041: train 67.126 val 66.992 test 67.793 (129.1 sec)\n",
      "> epoch 042: train 67.714 val 70.056 test 69.873 (129.3 sec)\n",
      "> epoch 043: train 66.482 val 67.937 test 67.640 (129.5 sec)\n",
      "> epoch 044: train 65.645 val 67.968 test 67.700 (129.3 sec)\n",
      "> epoch 045: train 64.912 val 63.312 test 63.948 (129.4 sec)\n",
      "> epoch 046: train 64.501 val 61.846 test 62.105 (129.6 sec)\n",
      "> epoch 047: train 63.085 val 59.622 test 59.730 (129.3 sec)\n",
      "> epoch 048: train 63.034 val 60.576 test 61.634 (129.8 sec)\n",
      "> epoch 049: train 62.665 val 67.321 test 67.539 (129.5 sec)\n",
      "> epoch 050: train 61.494 val 60.004 test 60.863 (129.2 sec)\n",
      "> epoch 051: train 60.791 val 63.803 test 63.477 (128.7 sec)\n",
      "> epoch 052: train 61.436 val 61.766 test 61.968 (129.3 sec)\n",
      "> epoch 053: train 60.505 val 60.997 test 60.991 (129.1 sec)\n",
      "> epoch 054: train 59.346 val 59.213 test 58.837 (128.8 sec)\n",
      "> epoch 055: train 58.768 val 62.219 test 62.556 (129.3 sec)\n",
      "> epoch 056: train 58.406 val 60.797 test 60.727 (129.1 sec)\n",
      "> epoch 057: train 58.690 val 59.228 test 59.488 (128.4 sec)\n",
      "> epoch 058: train 56.747 val 57.911 test 58.229 (129.1 sec)\n",
      "> epoch 059: train 56.675 val 59.524 test 59.985 (128.8 sec)\n",
      "> epoch 060: train 56.207 val 57.470 test 57.798 (129.2 sec)\n",
      "> epoch 061: train 55.717 val 60.424 test 60.922 (133.0 sec)\n",
      "> epoch 062: train 59.596 val 60.875 test 61.336 (131.5 sec)\n",
      "> epoch 063: train 55.247 val 56.287 test 57.060 (131.2 sec)\n",
      "> epoch 064: train 53.241 val 56.302 test 57.080 (129.4 sec)\n",
      "> epoch 065: train 53.961 val 54.368 test 54.968 (129.3 sec)\n",
      "> epoch 066: train 53.534 val 53.923 test 55.188 (129.6 sec)\n",
      "> epoch 067: train 52.862 val 55.099 test 55.285 (129.9 sec)\n",
      "> epoch 068: train 52.354 val 56.953 test 57.199 (129.2 sec)\n",
      "> epoch 069: train 52.408 val 54.065 test 54.480 (129.8 sec)\n",
      "> epoch 070: train 51.127 val 53.918 test 54.580 (130.0 sec)\n",
      "> epoch 071: train 50.979 val 53.321 test 53.334 (129.5 sec)\n",
      "> epoch 072: train 50.256 val 59.780 test 60.019 (129.5 sec)\n",
      "> epoch 073: train 49.983 val 53.352 test 53.418 (129.6 sec)\n",
      "> epoch 074: train 49.812 val 52.553 test 52.663 (129.5 sec)\n",
      "> epoch 075: train 48.880 val 53.443 test 53.871 (129.6 sec)\n",
      "> epoch 076: train 48.823 val 54.170 test 54.546 (129.9 sec)\n",
      "> epoch 077: train 48.135 val 52.268 test 51.889 (129.6 sec)\n",
      "> epoch 078: train 47.333 val 50.535 test 50.859 (129.6 sec)\n",
      "> epoch 079: train 47.341 val 52.215 test 52.218 (129.3 sec)\n",
      "> epoch 080: train 47.183 val 54.331 test 54.215 (129.6 sec)\n",
      "> epoch 081: train 46.681 val 54.430 test 54.507 (130.2 sec)\n",
      "> epoch 082: train 45.850 val 51.414 test 51.405 (129.0 sec)\n",
      "> epoch 083: train 45.652 val 53.123 test 54.028 (129.5 sec)\n",
      "> epoch 084: train 45.107 val 49.965 test 50.318 (129.5 sec)\n",
      "> epoch 085: train 45.141 val 52.667 test 52.849 (129.6 sec)\n",
      "> epoch 086: train 44.200 val 51.340 test 51.513 (129.7 sec)\n",
      "> epoch 087: train 43.885 val 50.109 test 50.733 (129.7 sec)\n",
      "> epoch 088: train 43.213 val 48.561 test 49.178 (129.5 sec)\n",
      "> epoch 089: train 43.140 val 51.591 test 52.348 (129.5 sec)\n",
      "> epoch 090: train 42.675 val 49.913 test 49.988 (129.0 sec)\n",
      "> epoch 091: train 42.287 val 53.444 test 53.365 (128.6 sec)\n",
      "> epoch 092: train 41.732 val 51.384 test 51.498 (129.3 sec)\n",
      "> epoch 093: train 41.674 val 49.959 test 49.888 (128.9 sec)\n",
      "> epoch 094: train 41.454 val 48.681 test 49.327 (129.1 sec)\n",
      "> epoch 095: train 40.921 val 47.756 test 47.914 (129.0 sec)\n",
      "> epoch 096: train 40.570 val 46.855 test 47.629 (129.3 sec)\n",
      "> epoch 097: train 39.946 val 47.722 test 48.092 (129.1 sec)\n",
      "> epoch 098: train 39.627 val 49.268 test 50.206 (129.2 sec)\n",
      "> epoch 099: train 39.524 val 46.946 test 47.380 (129.2 sec)\n",
      "> epoch 100: train 38.789 val 47.811 test 48.496 (129.3 sec)\n",
      "> epoch 101: train 38.576 val 47.079 test 47.424 (129.0 sec)\n",
      "> epoch 102: train 38.520 val 47.881 test 47.772 (129.2 sec)\n",
      "> epoch 103: train 38.001 val 45.849 test 45.888 (129.7 sec)\n",
      "> epoch 104: train 37.670 val 45.915 test 46.158 (129.5 sec)\n",
      "> epoch 105: train 36.921 val 47.202 test 47.994 (129.5 sec)\n",
      "> epoch 106: train 36.858 val 45.702 test 46.141 (129.3 sec)\n",
      "> epoch 107: train 36.694 val 46.784 test 46.852 (129.3 sec)\n",
      "> epoch 108: train 36.207 val 44.874 test 45.417 (129.6 sec)\n",
      "> epoch 109: train 35.907 val 44.810 test 45.487 (129.3 sec)\n",
      "> epoch 110: train 35.402 val 44.104 test 44.633 (129.1 sec)\n",
      "> epoch 111: train 35.277 val 47.284 test 47.899 (128.9 sec)\n",
      "> epoch 112: train 34.848 val 44.587 test 44.815 (129.5 sec)\n",
      "> epoch 113: train 34.630 val 44.212 test 44.581 (129.1 sec)\n",
      "> epoch 114: train 34.266 val 44.471 test 44.988 (129.8 sec)\n",
      "> epoch 115: train 34.123 val 43.409 test 43.736 (129.6 sec)\n",
      "> epoch 116: train 33.668 val 43.283 test 43.229 (129.4 sec)\n",
      "> epoch 117: train 33.438 val 42.970 test 43.207 (129.9 sec)\n",
      "> epoch 118: train 33.063 val 43.861 test 44.147 (133.2 sec)\n",
      "> epoch 119: train 32.862 val 43.818 test 43.902 (131.6 sec)\n",
      "> epoch 120: train 32.668 val 42.535 test 42.978 (130.2 sec)\n",
      "> epoch 121: train 32.223 val 42.871 test 43.306 (128.8 sec)\n",
      "> epoch 122: train 31.991 val 42.848 test 43.335 (129.1 sec)\n",
      "> epoch 123: train 31.701 val 42.958 test 43.494 (129.0 sec)\n",
      "> epoch 124: train 31.750 val 43.787 test 43.997 (129.1 sec)\n",
      "> epoch 125: train 31.113 val 42.118 test 42.530 (129.5 sec)\n",
      "> epoch 126: train 30.845 val 42.509 test 42.728 (129.3 sec)\n",
      "> epoch 127: train 30.685 val 44.603 test 44.636 (129.0 sec)\n",
      "> epoch 128: train 30.365 val 41.960 test 42.196 (128.8 sec)\n",
      "> epoch 129: train 29.969 val 42.077 test 42.580 (128.9 sec)\n",
      "> epoch 130: train 29.855 val 43.110 test 43.412 (128.9 sec)\n",
      "> epoch 131: train 29.713 val 41.466 test 42.278 (128.6 sec)\n",
      "> epoch 132: train 29.397 val 41.547 test 42.272 (129.1 sec)\n",
      "> epoch 133: train 28.904 val 40.892 test 41.408 (129.2 sec)\n",
      "> epoch 134: train 28.751 val 42.626 test 43.199 (129.1 sec)\n",
      "> epoch 135: train 28.554 val 42.423 test 43.014 (128.8 sec)\n",
      "> epoch 136: train 28.407 val 40.633 test 41.002 (128.6 sec)\n",
      "> epoch 137: train 28.171 val 41.521 test 41.962 (129.4 sec)\n",
      "> epoch 138: train 27.944 val 42.324 test 42.438 (129.2 sec)\n",
      "> epoch 139: train 27.691 val 40.551 test 40.864 (128.9 sec)\n",
      "> epoch 140: train 27.455 val 40.836 test 41.061 (129.0 sec)\n",
      "> epoch 141: train 27.116 val 41.422 test 41.898 (129.0 sec)\n",
      "> epoch 142: train 26.898 val 40.054 test 40.636 (129.0 sec)\n",
      "> epoch 143: train 26.912 val 40.251 test 40.951 (128.6 sec)\n",
      "> epoch 144: train 26.573 val 40.391 test 40.885 (128.9 sec)\n",
      "> epoch 145: train 26.311 val 40.630 test 40.808 (129.0 sec)\n",
      "> epoch 146: train 26.209 val 41.672 test 41.890 (128.8 sec)\n",
      "> epoch 147: train 25.769 val 39.734 test 40.300 (128.7 sec)\n",
      "> epoch 148: train 25.831 val 40.668 test 40.833 (129.0 sec)\n",
      "> epoch 149: train 25.531 val 40.296 test 40.886 (128.9 sec)\n",
      "> epoch 150: train 25.406 val 39.975 test 40.517 (129.0 sec)\n",
      "> epoch 151: train 25.131 val 40.978 test 41.431 (128.7 sec)\n",
      "> epoch 152: train 25.119 val 39.422 test 39.742 (128.9 sec)\n",
      "> epoch 153: train 24.814 val 39.254 test 39.963 (128.7 sec)\n",
      "> epoch 154: train 24.719 val 40.583 test 40.976 (129.3 sec)\n",
      "> epoch 155: train 24.253 val 39.204 test 39.565 (129.0 sec)\n",
      "> epoch 156: train 24.086 val 39.521 test 39.735 (129.0 sec)\n",
      "> epoch 157: train 24.074 val 40.663 test 41.024 (128.7 sec)\n",
      "> epoch 158: train 23.796 val 39.406 test 39.871 (128.8 sec)\n",
      "> epoch 159: train 23.735 val 40.998 test 41.259 (128.7 sec)\n",
      "> epoch 160: train 23.645 val 39.189 test 39.537 (129.1 sec)\n",
      "> epoch 161: train 23.479 val 39.951 test 40.246 (128.6 sec)\n",
      "> epoch 162: train 23.242 val 39.298 test 39.728 (128.5 sec)\n",
      "> epoch 163: train 23.073 val 39.072 test 39.509 (129.0 sec)\n",
      "> epoch 164: train 22.976 val 39.257 test 39.808 (128.5 sec)\n",
      "> epoch 165: train 22.804 val 39.253 test 39.733 (128.7 sec)\n",
      "> epoch 166: train 22.794 val 38.774 test 39.370 (128.6 sec)\n",
      "> epoch 167: train 22.457 val 39.013 test 39.510 (128.4 sec)\n",
      "> epoch 168: train 22.437 val 39.285 test 39.941 (128.6 sec)\n",
      "> epoch 169: train 22.331 val 38.987 test 39.419 (128.6 sec)\n",
      "> epoch 170: train 22.148 val 38.841 test 39.179 (129.1 sec)\n",
      "> epoch 171: train 22.125 val 39.155 test 39.573 (128.7 sec)\n",
      "> epoch 172: train 22.023 val 38.910 test 39.241 (128.6 sec)\n",
      "> epoch 173: train 21.916 val 39.011 test 39.468 (128.7 sec)\n",
      "> epoch 174: train 21.641 val 39.070 test 39.542 (129.1 sec)\n",
      "> epoch 175: train 21.631 val 38.731 test 39.215 (128.9 sec)\n",
      "> epoch 176: train 21.581 val 38.915 test 39.411 (128.7 sec)\n",
      "> epoch 177: train 21.565 val 38.575 test 39.072 (128.5 sec)\n",
      "> epoch 178: train 21.315 val 38.596 test 39.139 (128.5 sec)\n",
      "> epoch 179: train 21.423 val 39.148 test 39.496 (128.4 sec)\n",
      "> epoch 180: train 21.265 val 38.319 test 38.923 (128.7 sec)\n",
      "> epoch 181: train 21.220 val 38.781 test 39.268 (128.8 sec)\n",
      "> epoch 182: train 21.115 val 38.626 test 39.114 (129.0 sec)\n",
      "> epoch 183: train 21.172 val 38.548 test 38.992 (128.6 sec)\n",
      "> epoch 184: train 21.017 val 38.856 test 39.227 (128.7 sec)\n",
      "> epoch 185: train 20.917 val 38.602 test 39.058 (128.3 sec)\n",
      "> epoch 186: train 21.012 val 39.019 test 39.434 (128.4 sec)\n",
      "> epoch 187: train 20.896 val 38.495 test 38.985 (128.9 sec)\n",
      "> epoch 188: train 20.808 val 38.845 test 39.255 (128.5 sec)\n",
      "> epoch 189: train 20.853 val 38.603 test 39.057 (128.1 sec)\n",
      "> epoch 190: train 20.766 val 38.453 test 38.944 (128.4 sec)\n",
      "> epoch 191: train 20.687 val 38.586 test 39.044 (128.3 sec)\n",
      "> epoch 192: train 20.611 val 38.608 test 39.085 (128.6 sec)\n",
      "> epoch 193: train 20.729 val 38.535 test 39.031 (128.4 sec)\n",
      "> epoch 194: train 20.711 val 38.548 test 39.038 (128.5 sec)\n",
      "> epoch 195: train 20.801 val 38.582 test 39.076 (128.6 sec)\n",
      "> epoch 196: train 20.651 val 38.599 test 39.085 (128.4 sec)\n",
      "> epoch 197: train 20.553 val 38.596 test 39.087 (128.5 sec)\n",
      "> epoch 198: train 20.547 val 38.546 test 39.040 (129.0 sec)\n",
      "> epoch 199: train 20.637 val 38.548 test 39.040 (128.5 sec)\n"
     ]
    }
   ],
   "source": [
    "print('> start training')\n",
    "\n",
    "tr_ys = train_loader.dataset.data['homo'] \n",
    "me, mad = torch.mean(tr_ys), torch.mean(torch.abs(tr_ys - torch.mean(tr_ys)))\n",
    "\n",
    "if cuda:\n",
    "    me = me.cuda()\n",
    "    mad = mad.cuda()\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('> epoch %s:' % str(epoch).zfill(3), end=' ', flush=True) \n",
    "    start = time.time()\n",
    "\n",
    "    batch_train_loss = []\n",
    "    batch_val_loss = []\n",
    "    batch_test_loss = []\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = BatchGraph(batch, cuda, charge_scale)\n",
    "        \n",
    "        out = model(batch).reshape(-1)\n",
    "        #### **** YOUR CODE HERE **** #####\n",
    "        # compute l1-loss \n",
    "        loss = F.l1_loss(out, (batch.y-me)/mad)\n",
    "        # use (batch.y-me)/mad as the true value\n",
    "\n",
    "        loss.backward()\n",
    "        #### **** YOUR CODE HERE **** #####\n",
    "        # make step of opimizer \n",
    "        # google it\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss =  F.l1_loss(out*mad+me, batch.y)\n",
    "\n",
    "        batch_train_loss += [float(loss.data.cpu().numpy())]  \n",
    "        \n",
    "    train_loss += [np.mean(batch_train_loss)/0.001]\n",
    "    \n",
    "    print('train %.3f' % train_loss[-1], end=' ', flush=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            batch = BatchGraph(batch, cuda, charge_scale)\n",
    "            out = model(batch).reshape(-1)\n",
    "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
    "            batch_val_loss += [np.mean(loss)]\n",
    "            \n",
    "        val_loss += [np.mean(batch_val_loss)/0.001]\n",
    "        \n",
    "        print('val %.3f' % val_loss[-1], end=' ', flush=True)\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            batch = BatchGraph(batch, cuda, charge_scale)\n",
    "            out = model(batch).reshape(-1)\n",
    "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
    "            batch_test_loss += [np.mean(loss)]\n",
    "\n",
    "        test_loss += [np.mean(batch_test_loss)/0.001]\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    print('test %.3f (%.1f sec)' % (test_loss[-1], end-start), flush=True)\n",
    "    lr_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "graph_seminar_homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
